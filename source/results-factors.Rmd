---
title: "Results: Baseline"
author: "Maximilian Held"
output: pdf_document
library: held_library.bib
---

# Administration

<!--```{r administration, child='keyneson/keyneson.wiki/q-administration.md'}```-->
<!-- TODO MH: paste condition of instruction? -->
<!-- TODO MH include data-gathering here, too -->

<!-- COMMENT MH
notice: there may be stuff in the data that suggests how many factors make sense (loc 2287)
notice: I am definetely doing exploratory factor analysis; no ex-ante reason to expect one or the other result
make a correlation matrix, just for descriptives, look at which items are most highly correlated
same thing for q sorts
check kline 1994 and Harman 1976 in the above on PCA vs CFA
Harman 1976 says they will mostly be the same!
what we're loosing when then R2 increases is, by definition, some residuals.
interesting idea: maybe look at what kind of residuals was lost? could we tell a story about that?
-->

<!-- COMMENT MH
consider jonathan haidt, joshua greene on deep pragmatism and slow/fast morality
-->

Notice that some people complained about sorting several items, but that 0 is in fact an appropriate value.
For more on what 0 means to q, see Brown 1980 199.



# Data Gathered

The first CiviCon Citizen Conference on taxation produced a wealth of data, including:

- 2x 18x Q-Sorts (before, after)
- 2x 18x Item Feedback (before, after)
- 17x Socio-Economic & Demographic data
- 15x General written feedback
- 315x Notes, Posters, Illustrations
- 1601x Photos
- 100hrs Audio
- 80hrs Video (1080p)

<!-- TODO MCH: make this into a table with availability and status of processing -->


# Data Import

All citizens as well as the researcher and the two moderators completed two Q sorts each, one at the beginning and one at the end of the conference.


### Software for Open Science and Reproducible Research

Because scientific computing can do a great deal to harm or promote reproducible research and open access, some words on the choice of software will be in order.
<!-- TODO MCH: find some reproducible quote? what is it -->

Good software for the following q methodological analyses should fulfill the following criteria:

- *Reproducibility.*
  It should easy document and reproduce all the steps undertaken from raw data to the final factor interpretation.
  This will be especially important during the factor extraction and (indeterminate!) rotation phases, where consequential and sometimes controversial methodological decisions must be made.
- *Open Access.*
  Funded mostly by tax money, all of this scientific work should be conveniently available to the public.
  Research on deliberation and citizen participation, especially, should be *Open* Science.
- *Specialized for Q Methodology.*
  At its heart, Q methodology involves factor analyses and similar data reduction techniques, procedures that are widely used and available in all general-purpose statistics programs.
  However, Q methodology also requires some specialized operations, not easily accomplished in general-purpose programs.
  The transposed correlation matrix, flagging participants and compositing weighted factor scores in particular, are hard or counter-intuitive to do in mainstream software.
- *Programmatic Extensibility.*
  Counter to many Q studies, this research features several conditions (before, after), groups of participants (citizens, moderators, researcher), types of items (values, beliefs, preferences) as well as an extended research question, all of which will need to be analyzed systematically.
  Running and documenting all these variations will be next to impossible without programmatic extensibility of the software used.


The following free / open source and closed / proprietary programs are available:

| General-Purpose | Specialized
 -|----|----
 Closed Source | [SPSS](http://www-01.ibm.com/software/analytics/spss/), [STATA](http://www.stata.com) | [PCQ](http://www.pcqsoft.com)
 Open Source | [R](http://www.r-project.org) | R: [qmethod](https://github.com/aiorazabala/qmethod) package, [PQMethod](http://schmolck.userweb.mwn.de/qmethod/)

The closed-source, general-purpose programs offer some programmatic extensibility, but are poorly suited for Q methodology.
These commercial, and often expensive offerings also make it hard reproduce and open up research.
The special-purpose `PQMethod` was originally written for mainframe in 1992 by John Atkinson at Kent State University, and has since been ported for PCs and maintained by Peter Schmolck at the University of the Armed Forces in Munich, Germany.
<!-- TODO MCH: add citation for PQMethod -->
While nominally free and open-source, `PQMethod` consists largely of legacy FORTRAN code, which few people can read or write today.
`PQMethod` also offers little programmatic extensibility, because it is a standalone program running within layers of emulators.

The [`qmethod`](https://github.com/aiorazabala/qmethod) package (@Zabala-2014), implementing Q methodology in the free and open source [R programming language and software environment for statistical computing](http://www.r-project.org) (@R-2014) is the best fit for the above criteria.
R is freely available for all platforms, and supported by vibrant community of developers.
`Qmethod`, while released only recently, has been thoroughly validated (@Zabala-2014-a), leverages existing packages for extraction and rotation and conveniently wraps specialized Q functions.
Running inside the R command prompt, `qmethod` can be easily extended, now including several functions developed for this dissertation and contributed to the open source project.
<!-- TODO MCH add citation for my contribs -->
Most importantly, using `R`, every step from raw data to final factor interpretation can be traced back, using publicly available and vetted code, down to base functions.
<!-- TODO MCH find nice open source quote -->

#### Literate Programming

Machine-readable code, its results, and human-readable explanation of *what that code does* are often written and stored separately.
Not only does such separation invite mistakes when code, result and explanation diverge, but it also makes research harder to reproduce and is conceptually flawed.
At least in the context of statistical analysis, what we want a machine to do, why we want it, and to what effect are *one* intellectual operation, and should be presented as such.
Programs must not be considered black boxes that "do" things of their own accord, to be explained ex-post, but code should be a near equivalent of the same thought expressed in prose.

Donald Knuth has formulated this central tenet of his *Literate Programming* approach thus:

> Let us change our traditional attitude to the construction of programs:
> Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to humans what we want the computer to do.
> --- @Knuth-1984

I try to follow this approach by interweaving prose and code in one document, using the [`knitr`](http://yihui.name/knitr/) R package (@knitr).
Aside from published R packages, all code is run *inside* this document upon rendering.
Code will usually not be reproduced in print, but can always be inspected in the [source of this document](https://github.com/maxheld83/schumpermas), "*underneath*" the respective operation or result.


<!-- TODO MH: see timetable -->

```{r import-q-sorts}
q_sorts <- import.q.sorts(
  q.sorts.dir = "keyneson/qsorts/",
  q.set = q_set,
  q.distribution = q_distribution,
  conditions = c("before","after"),
  manual.lookup = as.matrix(
    read.csv(
      "keyneson/keyneson-sample/keyneson-concourse/ids.csv",
      row.names=2
    )
  )
)
```

## Codenames

```{r real-names}
real_names <- TRUE
if (real_names == TRUE) {
  codenames <- read.csv(file = "../Codenames.csv",header = TRUE)
  colnames(q_sorts) <- codenames$First.Name
}
```

## Participant Feedback


```{r import-q-feedback}
q_feedback <- import.q.feedback(
  q.feedback.dir = "keyneson/feedback/",
  q.sorts = q_sorts,
  q.set = q_set,
  manual.lookup = as.matrix(
    read.csv(
      "keyneson/keyneson-sample/keyneson-concourse/ids.csv",
      row.names=2
    )
  )
)
```

complete import of `r ncol(q_sorts)`, three participants


## Missing Data

```{r dropped-participants}
q_sorts <- q_sorts[ , !colnames(q_sorts) == "Wolfgang", ]  # delete researcher
q_sorts <- q_sorts[ , !colnames(q_sorts) == "Uwe", ]  # incomplete
q_sorts <- q_sorts[ , !colnames(q_sorts) == "Claus", ]  # incomplete
q_feedback <- q_feedback[ , !colnames(q_feedback) == "Wolfgang", ]  # delete researcher
q_feedback <- q_feedback[ , !colnames(q_feedback) == "Uwe", ]  # incomplete
q_feedback <- q_feedback[ , !colnames(q_feedback) == "Claus", ]  # incomplete
```

# Descriptives

```{r descriptives}
# item.mean <- apply(q.sorts, c(1,3), mean, na.rm = TRUE)
# item.mean <- cbind(item.mean, item.mean[,"after"] - item.mean[,"before"])
# colnames(item.mean)[3] <- "change"
# item.mean
# item.mean[order(item.mean[,"change"]),] #  sort by change

# item.sd <- apply(q.sorts, c(1,3), sd, na.rm = TRUE)
# item.sd <- cbind(item.sd, item.sd[,"after"] - item.sd[,"before"])
# colnames(item.sd)[3] <- "change"
# item.sd[order(item.sd[,"change"]),] #  sort
# item.sd

# person.cor <- cor(q.sorts[,,"before"])
# person.cor[which.min(abs(person.cor))]
# person.cor
# str(person.cor)
# help(which.min)
```

# Q Method Analysis



What kind of Q Methodological Analysis should I run?

- Deductive vs. inductive vs. abductive
- Theoretical / judgmental vs. objective
- Conservative vs. adventurous
- Holistic vs. atomized
- Baseline vs. treatment effect


## Correlations

All of the following analyses exctract patterns from a transposed correlation matrix.
Q studies do not always display or discuss the correlation matrix but considering that it is the basis for all downstream analyses, some initial comments will be in order.

`Qmethod` - as other Q software, does not report the correlation matrix, but the below correlation matrix is produced calling the same function also used in later analyses, a simple Pearson correlation coefficient.

```{r cor-viz-before}
source("../ggcorr/ggcorr.R")  # FIXME update ggally from CRAN, should at some point have this function
ggcorr(cor_matrix = cor(q_sorts[,,"before"]), label=TRUE, label_round=2)
```

```{r how many factors?}
# library(paran)
# hornvectors <- paran(t(q_sorts[,,"after"]),
#       iterations = 1000,x
#       centile = 95,
#       quietly = FALSE,
#       status = TRUE,
#       graph = TRUE,
#       color = TRUE)
```

<!-- COMMENT MH
read kline 1994 on wattss/stenner on PCA vs CFA, and they will be similar as per harman1976
-->
<!-- COMMENT MH
check whether the two flagging criteria make sense, aren't they too restrictive?
-->

<!-- COMMENT MH
via qlist Since Mary Furnari decided to rely on varimax rotation, Watts & Stenner (2012), for instance, would actually recommend PCA. (But note that Watts & Stenner erroneously think that varimax maximizes the amount of explained variance. To clarify: Varimax searches a simple structure solution characterized by a maximal number of either high or near zero loadings for every factor which is arrived at by maximizing the variance of the factors' loadings. The amount of explained variance is not affected by rotation.)
In my view, the meaningfulness of certain quantitative coefficients in Q, like so-called 'significance' of factor loadings is often overrated. So as if observing coefficient based rules could provide mathematical-statistical  proof for the soundness of the researcher's decisions and conclusions. Without referring to the wisdom of inferential statistics (which is not applicable in Q, IMO), however, I would dare to bet that given 60% explained variance of the 1st factor and 4% and less for the following, that Mary Furnari won't be able to assemble groups of sorts (to be flagged on different factors) that represent distinct = uncorrelated views. But that's just a bet which I possibly can lose. So nothing is lost by just trying out varimax solutions with 2, 3, 4 ... factors. Two simple (but not 100% unambiguous) rules for accepting a factor solution: (1) At least 2, better 3, defining sorts (load strongly on the respective factor only). (2) Intercorrelations of factor scores at a moderate level (possible choice of critical level: not higher than the size of the loading accepted for a defining sort).
-->

```{r analysis}
keyneson <- list("before"=c(), "after"=c())
keyneson$before <- qmethod(
  dataset = q_sorts[,,"before"],
  nfactors = 3,
  rotation = "varimax",
  forced = TRUE
)
# keyneson$after <- qmethod(
#   dataset = q_sorts[,,"after"],
#   nfactors = 3,
#   rotation = "varimax",
#   forced = TRUE
# )
```

```{r array-viz, fig.width = 20, fig.height=18}
arrayviz <- array.viz(
  QmethodRes = keyneson$before
  ,f.names = c("resentment","critical","moderate")
  ,incl.qdc = TRUE
  ,color.scheme = "Set1"
  ,extreme.labels = c("very much disagree","very much agree")
)
arrayviz[3]
```

<!--
stephenson on number of factors, via verena
OS-9-3-Stephenson.pdf S. 89 “Second, a little simple factor analysis is
all that the operations demand: It will be the end
% of work in this domain if anyone thinks that its be- all and end-all is factor analysis. The less of it, the better. Three or four factors are all that most well planned studies require; there's something loose in the works if anything like ten or so factors are carved out for interpretation. The key to sound work, i.e., to make discoveries, is what one puts into Q method as abduction, not what factor analysis turns out deductively.
-->

<!-- worry about bipolar factors -->

<!--
notice that I look at sd of pop, not sample, because it's not r stats
but still, dispersion is interesting - those are the loose lego blocks
is there maybe a need to also look at the loose lego blocks *overall*? And what are those? What are the loosest blocks?
-->

<!--
Frank destroys the SD of pro-socialism
-->

```{r frank discussion pro socialism}
# q.feedback["pro-socialism","Frank","after"]
```

```{r add-type}
# array.viz.data <- merge(  # add type of item
#   x = array.viz.data
#   ,y = q.sampling.structure  # that is where the types are from
#   ,by.x = 0  # these are rownames
#   ,by.y = "handle"  # that is how they are called
#   ,all = TRUE
# )
# rownames(array.viz.data) <- array.viz.data$Row.names  # restore rownames
# g <- g + geom_text(
#   aes(
#     ,fontface=c("plain","bold","italic")[metaconsensus]
#   )
#   ,size = 3.5
#)
#g <- g + scale_family_manual(c("serif","sans","mono"))
```

## Factor Extraction

<!-- COMMENT MH
agree that in fact, as Kampen and Tamas point out, the representativeness of the q sample to the universe of statements is a weak, weak spot.
Important: as cuppen in  kampen and tamas writes 3112 it's not the number of supporters that count, just the perspectives.
Consider the discussion of q methodology and validities oin kampen tamas 3112
do cluster analysis instead of factor; according to kampen and tamas. This might be interesting.
-->


### Which Data Reduction Technique

<!--
factor extraction
just use factanal normal factor
or normal princcop as opposed to psych
http://www.statmethods.net/advstats/factor.html
http://cran.r-project.org/web/packages/nFactors/index.html
http://cran.r-project.org/web/packages/FactoMineR/index.html
psych also appears to do parallel analysis by fa.parallel
-->

- *Exploratory* Factor Analysis (Centroid) @Brown1980's favorite
- Principal Components Analysis
- ...?


### How many factors?

<!-- for why validity makes no sense, see Brown 1980:4, that may (or may not) be so, but what definietely needs to be tested is whether, in fact, these tautologically valid viewpoints are also *patterned*, wheter some shared can be extracted.-->
In this (above context) Q methodologists often advise that, occasionally, researchers would know what to look for,
<!-- TODO MCH: find reference, find source from Watts Stenner on this -->
and that therefore some substantive discretion would be warranted.
In the extreme case, any number of extracted factor would be acceptable, so as long as it *made sense*.
The problem here, oddly not obvious to many Q researchers, is that this kind of "makes-sense"-discretion comes perilously close to the imposition of deductively hypothesized meaning so criticized in survey research. (see brown 3 or so)


- Experience / Common sense: 6--8 persons per ex-ante factor (Stenner \& Watts 2012: loc 2652):
    `r ncol(q_sorts)/8` factors
- Magic Number 7 (Brown 1980: 223):
    `r ncol(q_sorts)/7` people per factor?!
- Eigenvalue of nth factor > 1 (Kaiser 1960, Guttman 1954)
- Two or more loadings (Brown 1980: 222)
- Humphrey's rule: Cross-Product of 2 highest loadings greater than twice the standard error
- Scree plot (2nd derivative < 0)
- Parallel Analysis (Horn 1965)


<!-- look at communality -->

<!-- \cite[6]{Exel2005} recommends 4-5 people per viewpoint, which given 17 participants yields 3-4 factors, or viewpoints. -->

<!-- \cite{Wittenborn} says explicitly what the problem is: 132, namely that there is no test as to whether there are other people who would sort like this. -->

Brown's case (42) against statistical (as opposed to, ominously, "substantive") criteria for the number of cases seems to rest on particular people's viewpoints being important, which is not the case in my study, nor probably will it ever be in this kind of work.
This *does* make sense because you don't want to rest on numbers game along, namely the amount of people who load on a factor, that's true – but this is still a necessary, if not a sufficient condition.
Or something like that?

Brown (43) seems to be aware of the problem of false positives from Eigenvalues, but doesn't seem interested in parallel analysis.

He also argues (and more convincingly!) that (43) factor size depends on the people involved, and since the sampling of these people does not follow any kind of method, the result would be meaningless.
I think this is an overstatement; it merely implies false negatives?!? (or something)
Think this through!
Also, point out that absent any kind of such a criterion, we're left in a very uncomfortabel spot
also, in my case, arguably sampling *is* ok, because deliberative participants will always be self-selected (as argued elsewhere).
<!-- TODO MCH: notice in the recruitment part that a) too much money may drive out other incentives, and b) already, the VHS-kind-of-orientation-what-do-i-learn was a bit of a problem, might need greater stakes, better output format, more responsibility. This is all under the heading of external validity of the conference -->

### What Rotation?

- Judgmental / Manual
- Automatic (varimax, equamax etc.)


### Factor scores

<!-- Brown 34 explains the ideal type (his choice of words) analogy -->


### What visualization

notice that while z-scores have more information, it makes this hard to see because of overplotting.
A neat visualization, in this case, is more than just cosmetic; easy navigation can be considered crucial to arrive at good factor interpretations.

Notice that "consensus statements" is something of a misnomer, because the significance cutoff point is asymmetric.
<!-- TODO MCH: write to q list about this -->

